{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to PySpark\n",
    "A Novice Guide and Tips to setting up Spark locally and Running PySpark on AWS Elastic Map Reduce (EMR) cluster\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "Tom Marthaler  \n",
    "@tmarthal  \n",
    "DesertPy *2017-06-28*  \n",
    "https://github.com/tmarthal/notebooks/blob/master/Introduction%20to%20PySpark.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contents\n",
    "\n",
    "* Introduction to Spark \n",
    "* Spark architecture, data structures and operations\n",
    "* Setting up a local Spark development environment (findspark)\n",
    "* Sample PySpark commands / usages\n",
    "* Testing PySpark w/ Sparkling \n",
    "* Deploying PySpark applications \n",
    "* Command line interface to spin up an EMR cluster\n",
    "\n",
    "(Hard to figure out what to talk about - the spark ecosystem is huge) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Spark \n",
    "\n",
    "Overview ([source](http://spark.apache.org/docs/latest/))\n",
    "\n",
    "> Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "\n",
    "> It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Background\n",
    "\n",
    "> Spark, leverages in-memory primitives to improve performance over MapReduce for certain applications. It is well-suited to machine learning algorithms and interactive analytics. ([source](http://www.cio.com/article/3101842/analytics/databricks-unveils-commercial-support-for-apache-spark-2-0.html))\n",
    "\n",
    "* Originally developed at Berkeley's AMPLab in 2009.\n",
    "* BSD-ed in 2010.\n",
    "* Donated to Apache in 2013.\n",
    "* Apache Top-Level Project in 2014.\n",
    "* 1.0.0 released in May 2014.\n",
    "* 2.0.0 released in May 2016\n",
    "* Currently on 2.1.1 (released May 02, 2017).\n",
    "* Commerical support, certifications and backing by [Databricks](databricks.com)\n",
    "    - Databrick's seems to be pushing SparkSQL/DataFrames more than PySpark lately..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark Explained\n",
    "\n",
    "Apache Spark is a cluster computing platform designed to be fast and general-purpose. PySpark interfaces with the JVM to distribute the work to the cluster machines. \n",
    "\n",
    "Sparkâ€™s primary abstraction is a distributed collection of items called a __Resilient Distributed Dataset__ (RDD). \n",
    "\n",
    "\n",
    "![alt text](http://i.imgur.com/YlI8AqEl.png \"spark architecture\")\n",
    "([source](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> At its core, Spark is a â€œcomputational engineâ€ that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. Because the core engine of Spark is both fast and general-purpose, it powers multiple higher-level components specialized for various workloads, such as SQL or machine learning. These components are designed to inter-operate closely, letting you combine them like libraries in a software project.\n",
    "\n",
    "> At a high level, a Spark application consists of a driver program that launches various parallel operations on a cluster. The driver program contains the main function of your application which will be then distributed to the clusters members for execution. The SparkContext object is used by the driver program to access the computing cluster. For the shell applications the SparkContext is by default available through the sc variable.\n",
    "\n",
    "(cont.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> A very important concept in Spark is RDD â€“ resilient distributed data-set. This is an immutable collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. RDD can contain any type of object from Java, Scala, Python or R including user-defined classes. The RDDs can be created in two ways: by loading an external data-set or by distributing a collection of objects like list or sets.\n",
    "\n",
    "- Paraphrased from Chapter 1 *__Learning Spark__*, ([source](https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch01.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Operations\n",
    "\n",
    "After an RDD is created there are two types of operations:\n",
    "\n",
    "1. __Transformations__ â€“  construct a new RDD from an existing one\n",
    "1. __Actions__ â€“ compute a result based on an RDD\n",
    "\n",
    "Actions on RDDs are computed in a lazy way: given the chain of transformations, Spark can optimize the memory and compute required to calculate the result. Corollary to this, each time an action is run on an RDD, it is recomputed. Spark provides a `persist()` method to maintain an RDD for computing multiple actions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Programming Flow\n",
    "\n",
    "* create input RDDs\n",
    "* transform them using transformations\n",
    "* persist RDDs if needed for reuse\n",
    "* launch actions to start parallel computation\n",
    "* save the contents of the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark - Usage\n",
    "\n",
    "__Q:__ Do you need to use spark? \n",
    "\n",
    "__A:__ Â¯\\\\\\_(ãƒ„)\\_/Â¯ - but you'll most likely know when you can no longer use database for storage or single machine for processing\n",
    "\n",
    "\n",
    "Spark is more of an infrastructure than an application, and if you don't have a team to support a cluster installation, you can always use AWS or GCP.\n",
    "\n",
    "The tools exist, just have to give them a shot. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark - Requirements\n",
    "\n",
    "* Installation and Setup (the hardest part)\n",
    "    * Zipfian's Installation instructions (OSX/Linux) for help: https://github.com/zipfian/spark-install\n",
    "* AWS Account / AWS Keys (for running on Elastic Map Reduce) \n",
    "\n",
    "\n",
    "After (or during!) this talk go through the following pages:\n",
    "\n",
    "* Spark Quick Start Guide - https://spark.apache.org/docs/latest/quick-start.html \n",
    "* Spark Programming Guide - https://spark.apache.org/docs/2.1.1/programming-guide.html \n",
    "\n",
    "\n",
    "This talk is a summary of those two guides, plus random notes of implementing a PySpark system on EMR. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark \n",
    "\n",
    "Important things __not__ covered in this talk:\n",
    "\n",
    "* RDDs versus DataFrames \n",
    "    * \"DataSets\" are not supported by python (only in Java/Scala)\n",
    "* Batch versus Stream processing\n",
    "* No RDD persisting\n",
    "* Spark Shell `./bin/pyspark`\n",
    "    > note: the spark context variable is injected as 'sc' by default. easy to run examples.\n",
    "* No SparkSQL (using a SQL dialect for transformations, if that's your thing) - http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=udf\n",
    "\n",
    "\n",
    "The spark ecosystem is *huge*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running Locally\n",
    "\n",
    "After installing spark and the pyspark binaries (`pyspark` and `spark-submit`), also need to set up the `pyspark` package import path (part of the installation headaches)\n",
    "\n",
    "```\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. findspark does the latter.\n",
    "```\n",
    "Solution: use [`findspark`](https://github.com/minrk/findspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # this is going to fail\n",
    "    from pyspark import SparkContext\n",
    "except ImportError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use `findspark` package for easy path setup\n",
    "import findspark\n",
    "\n",
    "findspark.init() # add pyspark module to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ‘Œ\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# declare a context variable. when running `pyspark` from the command line,\n",
    "#  this `sc` variable is injected into the REPL\n",
    "# ðŸ™… sc = SparkContext() -- Don't construct\n",
    "sc = SparkContext.getOrCreate() # always get or create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RDD Examples\n",
    "RDDs can be created from python objects, using types of file input formats or by transforming other RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RDD from Python Objects\n",
    "\n",
    "[parallelize(c, numSlices=None):](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.parallelize)\n",
    "\n",
    "\n",
    "```\n",
    "Distribute a local Python collection to form an RDD. Using xrange is recommended if the input represents a range for performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Sum the squares of the integers from 1 to 10.\n",
    "# parallelize creates an RDD, map creates an RDD, sum returns a number\n",
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) \\\n",
    "    .map(lambda x: x**2) \\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDDs from Files  \n",
    "\n",
    "[textFile(name, minPartitions=None, use_unicode=True)](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.textFile)\n",
    "```\n",
    "Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings.\n",
    "```\n",
    "\n",
    "(From the API  http://spark.apache.org/docs/2.1.1/api/python/pyspark.html#pyspark.SparkContext.textFile) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Download a random csv file for later\n",
    "test_filename = '/opt/tmarthal/zoo-data.csv'\n",
    "!curl http://mlr.cs.umass.edu/ml/machine-learning-databases/zoo/zoo.data > {test_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Data definition http://mlr.cs.umass.edu/ml/machine-learning-databases/zoo/zoo.names\n",
    "!head -10 {test_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create an RDD from the local csv file with 3 partitions\n",
    "textFile = sc.textFile(test_filename, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Data definition http://mlr.cs.umass.edu/ml/machine-learning-databases/zoo/zoo.names\n",
    "\n",
    "```\n",
    "Attribute Information: (name of attribute and type of value domain)\n",
    "   1. animal name:      Unique for each instance\n",
    "   2. hair\t\tBoolean\n",
    "   3. feathers\t\tBoolean\n",
    "   4. eggs\t\tBoolean\n",
    "   5. milk\t\tBoolean\n",
    "   6. airborne\t\tBoolean\n",
    "   7. aquatic\t\tBoolean\n",
    "   8. predator\t\tBoolean\n",
    "   9. toothed\t\tBoolean\n",
    "  10. backbone\t\tBoolean\n",
    "  11. breathes\t\tBoolean\n",
    "  12. venomous\t\tBoolean\n",
    "  13. fins\t\tBoolean\n",
    "  14. legs\t\tNumeric (set of values: {0,2,4,5,6,8})\n",
    "  15. tail\t\tBoolean\n",
    "  16. domestic\t\tBoolean\n",
    "  17. catsize\t\tBoolean\n",
    "  18. type\t\tNumeric (integer values in range [1,7])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### textFile RDDs\n",
    "\n",
    "textFile() can take a list of locations, file globs and lots of other things\n",
    "\n",
    "\n",
    "* sc.textFile splits the file into lines (keep line sizes reasonable) \n",
    "* ideally one value per line, e.g. json structure or feature vector\n",
    "* use spark transforms to clean bad/malformed values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# project gutenburg raw text (.txt) files\n",
    "!ls /opt/tmarthal/example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# these RDDs should both be the same (didn't test the tar.gz one)\n",
    "books = sc.textFile('/opt/tmarthal/example/*.txt')\n",
    "books = sc.textFile(\"/opt/tmarthal/example/beowulf.txt,/opt/tmarthal/example/embroidery.txt\")\n",
    "\n",
    "\n",
    "# Can also read in compressed files and operate on them directly\n",
    "#  (less logging though, since not processed line by line)\n",
    "books_gz = sc.textFile(\"/opt/tmarthal/*.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other Data Formats\n",
    "\n",
    "* If you control the data format, *parquet* has been shown to improve I/O performance\n",
    "* __ Spark can also read directly from S3__\n",
    "* `textFile()` can also read directly from hdfs:// and other storage systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operators In Use\n",
    "RDDs have **actions**, which return values, and **transformations**, which return pointers to new RDDs.\n",
    "\n",
    "There are a lot of functional operators, beyond just `map()` and `reduce()`. Check the PySpark API for the full list : http://spark.apache.org/docs/2.1.0/api/python/pyspark.html\n",
    "\n",
    "[pyspark-pictures](http://nbviewer.jupyter.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb)\n",
    "- neat conceptual view of the PySpark operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PySpark Transformations \n",
    "\n",
    "All the normal functional transforms:\n",
    "\n",
    "* `filter` \n",
    "* `flatMap`\n",
    "* `groupBy`\n",
    "* `map`\n",
    "* `sample`\n",
    "* `sortBy`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`flatMap` visualization example : \n",
    "\n",
    "![flatmap](https://github.com/jkthompson/pyspark-pictures/raw/master/images/readme-example.png \"flatmap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PySpark Actions\n",
    "These are methods instantiate the RDD processing. \n",
    "\n",
    "* count\n",
    "* countApprox\n",
    "* reduce\n",
    "* reduceByKey\n",
    "* sum\n",
    "\n",
    "Also actions that transform the RDD into Python data types\n",
    "\n",
    "* collect\n",
    "* first\n",
    "* top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "textFile.first() # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "textFile.count() # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# RDD -> RDD -> RDD -> RDD -> python list\n",
    "animalsWithHairList = textFile \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .filter(lambda line: line[1] == '1') \\\n",
    "    .map(lambda line: line[0]) \\\n",
    "    .collect() \n",
    "print(animalsWithHairList, end=\" \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Save RDD\n",
    "\n",
    "* `.saveAsTextFile` persists the transformed RDD into a given location\n",
    "    * Will write to local file, s3 or any other format that can be read in\n",
    "    * the format of the RDD/textfile is up to the developer\n",
    "\n",
    "\n",
    "* Also `saveAsHadoopFile`, `saveAsSequenceFile` and other formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# clear the output location - spark will not overwrite contents (org.apache.hadoop.mapred.FileAlreadyExistsException)\n",
    "!rm -Rf /opt/tmarthal/animals_with_hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "animalsWithHair = textFile \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .filter(lambda line: line[1] == '1') \\\n",
    "    .map(lambda line: line[0])\n",
    "    \n",
    "# since there were 3 minimum partitions in the textfile, 3 \n",
    "animalsWithHair.saveAsTextFile('/opt/tmarthal/animals_with_hair/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!ls /opt/tmarthal/animals_with_hair/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!cat /opt/tmarthal/animals_with_hair/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing PySpark Applications\n",
    "\n",
    "Do's & Don'ts: \n",
    "\n",
    "* make sure to write testable python (Spark jobs tend towards single file/functions)\n",
    "   - inject the spark context into a callable method (no logic in `__main__` or bare)\n",
    "   - move common code to modules/packages\n",
    "\n",
    "\n",
    "* encapsulate transforms into named functions, only use lamdas for simple operations\n",
    "   - the hardest thing to understand about some spark code is knowing the shape of the transformed RDD\n",
    "   - pyspark has dynamically typed RDDs, definitely need to test their structure/contents\n",
    "\n",
    "\n",
    "* can use `unittest` just like other python codebases\n",
    "   - `unittest` and `mock` the named functions directly for >95% coverage \n",
    "\n",
    "\n",
    "* `pysparking` for integration tests, creates an in-memory python `pysparkling.Context`\n",
    "   - it was developed for running spark applications on small data, no JVM/Hadoop https://github.com/svenkreiss/pysparkling\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ðŸ™… bad - from example https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ðŸ‘Œ good - separated method\n",
    "def pi(context, partitions):\n",
    "    n = 100000 * partitions\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "    return context.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    \n",
    "    count = pi(spark.sparkContext, partitions)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Testable integration with pysparkling\n",
    "from pysparkling import Context\n",
    "\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "def pi(context, n,  partitions):\n",
    "    \n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "    return context.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "\n",
    "\n",
    "# \"integration test\" code\n",
    "partitions = 2\n",
    "n = 100000 * partitions\n",
    "c = pi(Context(), n, partitions)\n",
    "pi = (4.0 * c / n)\n",
    "\n",
    "# then assert as normal in tests\n",
    "assert (pi > 3 and pi < 3.2)  # self.assertGreater(pi, 3) etc.\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deploying PySpark Applications\n",
    "\n",
    "When running a spark application from a command line, the syntax is as follows:\n",
    "\n",
    "```\n",
    "> spark-submit --master=local[4] /opt/tmarthal/wordcount-example.py /opt/tmarthal/example/beowulf.txt\n",
    "                 ^^ cluster             ^^^^ script file               ^^^ argument\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Packaging / depedency management is a huge headache - there are two ways to package dependencies \n",
    "\n",
    "1. Use the `--py-files` argument to the spark-submit command\n",
    "   * Example: `spark-submit --py-files dependency.py,library.zip main.py argument1 argument2`\n",
    "   * Does not work with compiled python libraries, e.g. numpy\n",
    "   * More dynamic, but may impact import statements\n",
    "\n",
    "   \n",
    "2. Install the files into the python path on the cluster machines\n",
    "   * More dev-ops/operations management than application development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deploying to an EMR\n",
    "\n",
    "The `spark-submit` call in an EMR system is the same as tested locally, but there is no filesystem. All configuration and code must be read from s3 or setup directly with the EMR cluster.\n",
    "\n",
    "\n",
    "The idea behind this sample analysis is to count the top 100 words in the book files in a `data` folder.\n",
    "\n",
    "> `spark-submit -master=local wordcount.py s3n://tmarthal-spark/data/*.txt s3n://tmarthal-spark/top-words` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For purposes of illustration, presume that the s3 bucket `tmarthal-spark` has the following files:\n",
    "\n",
    "```\n",
    "tmarthal-spark/\n",
    "  - wordcount.py\n",
    "  - tmarthal-1.0.1.tar.gz\n",
    "  - emr-bootstrap.sh\n",
    "  - requirements.txt\n",
    "  - spark-env.json\n",
    "  - data/\n",
    "    - beowulf.txt\n",
    "    - embroidery.txt\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 ls tmarthal-spark --recursive --profile tmarthal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* `wordcount.py` - a pyspark application for counting words, two arguments the location of the text files and the location to save the output words\n",
    "\n",
    "* `tmarthal-1.0.1.tar.gz` - the source distribution of shared python code\n",
    "> i.e. from `python3.4 setup.py sdist`\n",
    "\n",
    "* ` emr-bootstrap.sh` - a bash file that is run on each cluster machine, to install the python dependencies\n",
    "\n",
    "* `requirements.txt` - a typical pip requirements file\n",
    "\n",
    "* `spark-env.json` - the EMR environment configuration to use python3.4 (instead of the default 2.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# spark cluster bootstrap file to install the python requirements\n",
    "\n",
    "# pip can't install from s3 urls, so we presign the https url for the python code in s3 and install it using the\n",
    "# temporary authorized url(s)\n",
    "#Note: If this url returns a 403: Forbidden error, then need to modify the bucket permissions\n",
    "\n",
    "# The requirements\n",
    "aws s3 presign tmarthal-spark/requirements-spark.txt | xargs sudo pip-3.4 install -r\n",
    "\n",
    "# any distributed source modules\n",
    "aws s3 presign tmarthal-spark/tmarthal-1.0.1.tar.gz --expires-in 30 | xargs sudo pip-3.4 install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "boto3==1.4.4\n",
    "botocore==1.5.29\n",
    "docutils==0.13.1\n",
    "findspark==1.1.0\n",
    "jmespath==0.9.2\n",
    "numpy==1.11.3\n",
    "psutil==5.2.0\n",
    "python-dateutil==2.6.0\n",
    "s3transfer==0.1.10\n",
    "scikit-learn==0.18.1\n",
    "scipy==0.17.0\n",
    "six==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"Classification\": \"spark-env\",\n",
    "        \"Properties\": {},\n",
    "        \"Configurations\": [\n",
    "            {\n",
    "                \"Classification\": \"export\",\n",
    "                \"Properties\": {\n",
    "                    \"PYSPARK_PYTHON\": \"python34\"\n",
    "                },\n",
    "                \"Configurations\": []\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EMR Cluster Creation\n",
    "\n",
    "Example EMR aws-cli cluster creation and processing single step spark process:\n",
    "\n",
    "```\n",
    "> aws emr create-cluster \\\n",
    "  --name \"Gutenburg Top Words\" \\\n",
    "  --release-label emr-5.4.0 \\\n",
    "  --applications Name=Spark \\\n",
    "  --instance-type m3.xlarge \\\n",
    "  --instance-count 1 \\\n",
    "  --steps Type=spark,Name=FilterHouseholds,Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.yarn.submit.waitAppCompletion=true,--num-executors,2,s3://tmarthal-spark/wordcount.py,s3n://tmarthal-spark/data/*.txt,s3n://tmarthal-spark/top-words],ActionOnFailure=TERMINATE_CLUSTER \\\n",
    "  --use-default-roles \\\n",
    "  --configurations https://s3.amazonaws.com/tmarthal-spark/spark-env.json \\\n",
    "  --bootstrap-action Path=s3://tmarthal-spark/emr-bootstrap.sh,Args=[] \\\n",
    "  --region us-east-1 \\\n",
    "  --log-uri 's3://tmarthal-spark/example/logs' \\\n",
    "  --auto-terminate\n",
    "```\n",
    "\n",
    "This will launch a cluster of 2 machines (`--num-executors`) and process all of the .txt files. \n",
    "\n",
    "For a full explanation of the EMR api, see http://docs.aws.amazon.com/cli/latest/reference/emr/create-cluster.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> My personal account's security groups are not setup to run this properly at the moment. :(\n",
    "\n",
    "\n",
    "The `wordcount.py` that reads in the text files and outputs the words is left to the reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* [Main] Tim Hopper's PySpark Presentation - https://github.com/tdhopper/rta-pyspark-presentation/blob/master/slides.ipynb\n",
    "* Best Practices Writing Production PySpark Jobs - https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f\n",
    "* Using Amazon Elastic Map Reduce (EMR) with Spark and Python 3.4 http://blog.thehumangeo.com/amazon-emr-spark-python3.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions / Commentary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
